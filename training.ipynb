{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0430e7f",
   "metadata": {},
   "source": [
    "### Dependencies Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d693eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ef6276f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded: 0.flac | Sample Rate: 16000 | Duration: 3.30s\n",
      "✅ Loaded: 1.flac | Sample Rate: 16000 | Duration: 3.66s\n",
      "✅ Loaded: 10.flac | Sample Rate: 16000 | Duration: 2.02s\n",
      "✅ Loaded: 11.flac | Sample Rate: 16000 | Duration: 2.02s\n",
      "✅ Loaded: 12.flac | Sample Rate: 16000 | Duration: 1.78s\n",
      "✅ Loaded: 13.flac | Sample Rate: 16000 | Duration: 1.98s\n",
      "✅ Loaded: 14.flac | Sample Rate: 16000 | Duration: 2.22s\n",
      "✅ Loaded: 15.flac | Sample Rate: 16000 | Duration: 2.02s\n",
      "✅ Loaded: 16.flac | Sample Rate: 16000 | Duration: 1.94s\n",
      "✅ Loaded: 17.flac | Sample Rate: 16000 | Duration: 2.38s\n",
      "✅ Loaded: 18.flac | Sample Rate: 16000 | Duration: 3.78s\n",
      "✅ Loaded: 19.flac | Sample Rate: 16000 | Duration: 1.70s\n",
      "✅ Loaded: 2.flac | Sample Rate: 16000 | Duration: 2.42s\n",
      "✅ Loaded: 20.flac | Sample Rate: 16000 | Duration: 1.74s\n",
      "✅ Loaded: 21.flac | Sample Rate: 16000 | Duration: 2.22s\n",
      "✅ Loaded: 22.flac | Sample Rate: 16000 | Duration: 2.26s\n",
      "✅ Loaded: 23.flac | Sample Rate: 16000 | Duration: 1.82s\n",
      "✅ Loaded: 24.flac | Sample Rate: 16000 | Duration: 2.10s\n",
      "✅ Loaded: 25.flac | Sample Rate: 16000 | Duration: 2.98s\n",
      "✅ Loaded: 26.flac | Sample Rate: 16000 | Duration: 2.74s\n",
      "✅ Loaded: 27.flac | Sample Rate: 16000 | Duration: 1.82s\n",
      "✅ Loaded: 28.flac | Sample Rate: 16000 | Duration: 3.14s\n",
      "✅ Loaded: 29.flac | Sample Rate: 16000 | Duration: 2.78s\n",
      "✅ Loaded: 3.flac | Sample Rate: 16000 | Duration: 5.10s\n",
      "✅ Loaded: 4.flac | Sample Rate: 16000 | Duration: 3.62s\n",
      "✅ Loaded: 5.flac | Sample Rate: 16000 | Duration: 3.18s\n",
      "✅ Loaded: 6.flac | Sample Rate: 16000 | Duration: 4.58s\n",
      "✅ Loaded: 7.flac | Sample Rate: 16000 | Duration: 3.70s\n",
      "✅ Loaded: 8.flac | Sample Rate: 16000 | Duration: 1.82s\n",
      "✅ Loaded: 9.flac | Sample Rate: 16000 | Duration: 2.30s\n"
     ]
    }
   ],
   "source": [
    "alexa_dir = 'data/alexa'\n",
    "\n",
    "for filename in os.listdir(alexa_dir):\n",
    "    if filename.endswith('.flac'):\n",
    "        file_path = os.path.join(alexa_dir, filename)\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None)  # sr=None keeps the original sample rate\n",
    "            print(f\"✅ Loaded: {filename} | Sample Rate: {sr} | Duration: {len(y)/sr:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Could not load {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "272110ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded: 00a97647-55b9-4f62-be20-8e4b0ee510b0.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 00aba123-ae3a-4e0a-8603-9f7277b7d41f.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 00af045b-ead8-4379-9110-c038e0bdd855.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 00c476c2-9e01-40df-96c8-a5153db5b157.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 01f13214-d1eb-48b6-8f58-e397731158fd.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 01f46f97-7ea8-474a-9285-13b038477eec.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 0b601785-a157-4263-920c-7890e3564efc.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 0c20da21-b168-470d-9b99-98e2161ed91a.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 0c7910b3-4414-4d48-9e6f-292a1d099f6f.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 0d7cfa1f-6de3-42f4-97e1-324cda1d1271.wav | Sample Rate: 16000 | Duration: 2.30s\n",
      "✅ Loaded: 0df7b7ff-d667-488d-b4c2-ea78b4b847fe.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 0e165e17-134f-4cee-9ec6-b43d1412d7d7.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 0eaa1c84-950f-44d1-88f1-f000c3e6e4ba.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 0eb17b69-7f9f-4db5-b68b-2c365cbdd660.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 0f597704-ff80-4c39-9ef4-275d70461617.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 0fcb739e-f07c-46df-8108-747aef697f63.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 1a617bc6-f87c-43d7-a0af-d482476276c5.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 1c7fce2f-1bb6-4df8-8dda-644f2159047f.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 1c9eaae2-1f5f-48bc-8379-6d1e1943f50b.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 1c9fda58-050b-4b8f-8289-dd08c3107c8c.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 1d3548b5-a5e3-407a-96bb-502037d3b2b8.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 1dd67e92-a034-4afd-b28e-5bf9861db753.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 1ddadbc1-4d52-4df8-88c2-8615587e2608.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 1e128153-2111-4a6f-8792-6c24220ee5b0.wav | Sample Rate: 16000 | Duration: 2.53s\n",
      "✅ Loaded: 1f3a351b-7bc8-4175-8b1d-1e5a9fea2a92.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 1f549862-de22-4af0-875d-7ff927d127c2.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 1f79ab12-639a-4d89-9bcc-704b1119e4e2.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 1ff2cc53-9fcd-442d-a939-0a3df41c8327.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 1ff6be5d-7958-464a-8726-1c268071d77d.wav | Sample Rate: 16000 | Duration: 3.07s\n",
      "✅ Loaded: 2a713afc-d208-4eab-9dc1-fcdd862d8db6.wav | Sample Rate: 16000 | Duration: 3.07s\n"
     ]
    }
   ],
   "source": [
    "jarvis_dir = 'data/jarvis'\n",
    "\n",
    "for filename in os.listdir(jarvis_dir):\n",
    "    if filename.endswith('.wav'):\n",
    "        file_path = os.path.join(jarvis_dir, filename)\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None)  # sr=None preserves original sampling rate\n",
    "            print(f\"✅ Loaded: {filename} | Sample Rate: {sr} | Duration: {len(y)/sr:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Could not load {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b1b17",
   "metadata": {},
   "source": [
    "### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33c25b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'filepath': 'data\\\\alexa\\\\0.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\1.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\10.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\11.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\12.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\13.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\14.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\15.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\16.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\17.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\18.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\19.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\2.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\20.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\21.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\22.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\23.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\24.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\25.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\26.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\27.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\28.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\29.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\3.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\4.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\5.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\6.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\7.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\8.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\alexa\\\\9.flac', 'wake_word': 'alexa'}, {'filepath': 'data\\\\jarvis\\\\00a97647-55b9-4f62-be20-8e4b0ee510b0.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\00aba123-ae3a-4e0a-8603-9f7277b7d41f.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\00af045b-ead8-4379-9110-c038e0bdd855.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\00c476c2-9e01-40df-96c8-a5153db5b157.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\01f13214-d1eb-48b6-8f58-e397731158fd.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\01f46f97-7ea8-474a-9285-13b038477eec.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\0b601785-a157-4263-920c-7890e3564efc.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\0c20da21-b168-470d-9b99-98e2161ed91a.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\0c7910b3-4414-4d48-9e6f-292a1d099f6f.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\0d7cfa1f-6de3-42f4-97e1-324cda1d1271.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\0df7b7ff-d667-488d-b4c2-ea78b4b847fe.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\0e165e17-134f-4cee-9ec6-b43d1412d7d7.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\0eaa1c84-950f-44d1-88f1-f000c3e6e4ba.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\0eb17b69-7f9f-4db5-b68b-2c365cbdd660.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\0f597704-ff80-4c39-9ef4-275d70461617.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\0fcb739e-f07c-46df-8108-747aef697f63.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\1a617bc6-f87c-43d7-a0af-d482476276c5.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\1c7fce2f-1bb6-4df8-8dda-644f2159047f.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\1c9eaae2-1f5f-48bc-8379-6d1e1943f50b.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\1c9fda58-050b-4b8f-8289-dd08c3107c8c.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\1d3548b5-a5e3-407a-96bb-502037d3b2b8.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\1dd67e92-a034-4afd-b28e-5bf9861db753.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\1ddadbc1-4d52-4df8-88c2-8615587e2608.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\1e128153-2111-4a6f-8792-6c24220ee5b0.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\1f3a351b-7bc8-4175-8b1d-1e5a9fea2a92.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\1f549862-de22-4af0-875d-7ff927d127c2.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\1f79ab12-639a-4d89-9bcc-704b1119e4e2.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\1ff2cc53-9fcd-442d-a939-0a3df41c8327.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\1ff6be5d-7958-464a-8726-1c268071d77d.wav', 'wake_word': 'jarvis'}, {'filepath': 'data\\\\jarvis\\\\2a713afc-d208-4eab-9dc1-fcdd862d8db6.wav', 'wake_word': 'jarvis'}]\n",
      "             filepath wake_word\n",
      "0   data\\alexa\\0.flac     alexa\n",
      "1   data\\alexa\\1.flac     alexa\n",
      "2  data\\alexa\\10.flac     alexa\n",
      "3  data\\alexa\\11.flac     alexa\n",
      "4  data\\alexa\\12.flac     alexa\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"data\"\n",
    "data = []\n",
    "\n",
    "for wake_word in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, wake_word)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if (wake_word == 'alexa' and filename.endswith('.flac')) or \\\n",
    "               (wake_word == 'jarvis' and filename.endswith('.wav')):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                data.append({'filepath': file_path, 'wake_word': wake_word})\n",
    "\n",
    "# print(data)\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd276e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wake_word\n",
       "alexa     30\n",
       "jarvis    30\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define 2 labels\n",
    "df['wake_word'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a652fe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    30\n",
      "1    30\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>wake_word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data\\alexa\\0.flac</td>\n",
       "      <td>alexa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data\\alexa\\1.flac</td>\n",
       "      <td>alexa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data\\alexa\\10.flac</td>\n",
       "      <td>alexa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data\\alexa\\11.flac</td>\n",
       "      <td>alexa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data\\alexa\\12.flac</td>\n",
       "      <td>alexa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             filepath wake_word  label\n",
       "0   data\\alexa\\0.flac     alexa      0\n",
       "1   data\\alexa\\1.flac     alexa      0\n",
       "2  data\\alexa\\10.flac     alexa      0\n",
       "3  data\\alexa\\11.flac     alexa      0\n",
       "4  data\\alexa\\12.flac     alexa      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['wake_word'].apply(lambda x: 0 if x == 'alexa' else 1)\n",
    "print(df['label'].value_counts())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f4054",
   "metadata": {},
   "source": [
    "### Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caf0fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_duration = 2.0\n",
    "sample_rate = 16000\n",
    "n_mfcc = 13\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_features(path):\n",
    "    y, sr = librosa.load(path, sr=sample_rate, duration=max_duration)\n",
    "    expected_len = int(max_duration * sample_rate)\n",
    "    y = librosa.util.fix_length(y, size=expected_len)  # ← Fixed this line\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfcc, axis=1)\n",
    "\n",
    "# Extract features for each file\n",
    "features = []\n",
    "valid_labels = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        mfcc_feat = extract_features(row['filepath'])\n",
    "        features.append(mfcc_feat)\n",
    "        valid_labels.append(row['label'])\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {row['filepath']}: {e}\")\n",
    "\n",
    "# Convert to arrays\n",
    "X = np.array(features)\n",
    "y = np.array(valid_labels)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test sets (70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea6a9dc",
   "metadata": {},
   "source": [
    "### Train Wake Word Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695a6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 0.9444444444444444\n",
      "📊 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       alexa       0.90      1.00      0.95         9\n",
      "      jarvis       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.94        18\n",
      "   macro avg       0.95      0.94      0.94        18\n",
      "weighted avg       0.95      0.94      0.94        18\n",
      "\n",
      "🧾 Confusion Matrix:\n",
      " [[9 0]\n",
      " [1 8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "clf = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb677d",
   "metadata": {},
   "source": [
    "### Evaluate the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ef6264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 0.9444444444444444\n",
      "📊 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       alexa       0.90      1.00      0.95         9\n",
      "      jarvis       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.94        18\n",
      "   macro avg       0.95      0.94      0.94        18\n",
      "weighted avg       0.95      0.94      0.94        18\n",
      "\n",
      "🧾 Confusion Matrix:\n",
      " [[9 0]\n",
      " [1 8]]\n",
      "+----------------------+---------+----------+\n",
      "| Predicted \\ Actual   |   Alexa |   Jarvis |\n",
      "+======================+=========+==========+\n",
      "| Alexa                |       9 |        0 |\n",
      "+----------------------+---------+----------+\n",
      "| Jarvis               |       1 |        8 |\n",
      "+----------------------+---------+----------+\n",
      "\n",
      "✅ Overall Accuracy: 94.44%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "print(\"✅ Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"📊 Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['alexa', 'jarvis']))\n",
    "print(\"🧾 Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "\n",
    "# Create table for tabulate\n",
    "headers = [\"Predicted \\\\ Actual\", \"Alexa\", \"Jarvis\"]\n",
    "table = [\n",
    "    [\"Alexa\", cm[0][0], cm[0][1]],\n",
    "    [\"Jarvis\", cm[1][0], cm[1][1]]\n",
    "]\n",
    "\n",
    "# Display\n",
    "print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
    "print(f\"\\n✅ Overall Accuracy: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
